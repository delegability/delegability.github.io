<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>AI Task Delegability - About</title>
    <!-- bootstrap -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link rel="stylesheet" href="css/style.css">

    <!-- jquery & popper for bootstrap, may remove if not needed -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
    <!-- d3 -->
    <script src="https://d3js.org/d3.v4.js"></script>

    <script src="js/search.js"></script>
    <script src="js/load_task.js"></script>
    <script src="js/survey.js"></script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134130813-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-134130813-1');
    </script>
  </head>

  <body>
    <nav class="navbar navbar-expand-sm navbar-dark bg-dark">
      <a class="navbar-brand" href="#">AI Task Delegability</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggler" aria-controls="navbarToggler" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarToggler">
        <div class="navbar-nav mr-auto mt-2 mt-lg-0">
          <a class="nav-item nav-link" href="index.html">Home</a>
          <a class="nav-item nav-link active" href="#">About<span class="sr-only">(current)</span></a>
          <a class="nav-item nav-link" href="table.html">Data</a>
          <a class="nav-item nav-link" href="plots.html">Visualization</a>
          <a class="nav-item nav-link" href="survey.html">Contribute</a>
        </div>
      </div>
    </nav>

    <div class="container fixed">
      <h1>About</h1>

      <h3>Abstract</h3>
      <p>Although artificial intelligence holds promise for addressing societal challenges, issues of exactly which tasks to automate and the extent to do so remain understudied. We approach the problem of task delegability from a human-centered perspective by developing a framework on human perception of task delegation to artificial intelligence.
We consider four high-level factors that can contribute to a delegation decision: motivation, difficulty, risk, and trust.
To obtain an empirical understanding of human preferences in different tasks, we build a dataset of 100 tasks from academic papers, popular media portrayal of AI, and everyday life. For each task, we administer a survey to collect judgments of each factor and ask subjects to pick the extent to which they prefer AI involvement. 
We find little preference for full AI control and a strong preference for machine-in-the-loop designs, in which humans play the leading role. 
Our framework can effectively predict human preferences in the degree of AI assistance.
Among the four factors, trust is the most predictive of human preferences of optimal human-machine delegation.
In all, this work is a first step towards quantifying the delegability of traditionally-human tasks to AI. We hope to highlight this as an important research area for human-centered computing.

      <h3>A Framework for Task Delegability</h3>

      <table class="table table-hover table-sm" style="border:none">
        <caption>An overview of the four factors in our AI task delegability framework.</caption>
        <thead>
          <tr>
            <th scope="col">#</th>
            <th scope="col">Factors</th>
            <th scope="col">Components</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <th scope="row">1</th>
            <td>Motivation</th>
            <td>Intrinsic motivation, goals, utility</td>
          </tr>
          <tr>
            <th scope="row">2</th>
            <td>Difficulty</th>
            <td>Social skills, creativity, effort required, expertise required, human ability</td>
          </tr>
          <tr>
            <th scope="row">3</th>
            <td>Risk</td>
            <td>Accountability, uncertainty, impact</td>
          </tr>
          <tr>
            <th scope="row">4</th>
            <td>Trust</td>
            <td>Machine ability, interpretability, value alignment</td>
          </tr>
        </tbody>
      </table>

      <p>
      To explain human preferences of task delegation to AI, we
      develop a framework with four factors: a person’s <b>motivation</b> in undertaking the task, their perception of the task’s
      <b>difficulty</b>, their perception of the <b>risk</b> associated with accomplishing the task, and finally their <b>trust</b> in the AI agent.
      </p>
      <p>
      We choose these factors because motivation, difficulty, and
      risk respectively cover why a person chooses to perform a
      task, the process of performing a task, and the outcome,
      while trust captures the interaction between the person and
      the AI. 
      </p>

      <div style="margin:auto">
      <figure class="figure" style="margin:auto;text-align:center;display:block;">
        <img src="images/model.png" class="figure-img img-fluid" alt="The four-factor representation of our framework on task delegation decisions.">
        <figcaption class="figure-caption">Hypothesized four factors for task delegability under our framework.<figcaption>
      </figure>
      </div>

      <h5>Motivation</h5>
      <p>
      Motivation is an energizing factor that helps initiate, sustain, and regulate task-related actions by directing our attention towards goals or values. 
      Affective (emotional) and cognitive processes are thought to be collectively responsible for driving action, so we consider <b>intrinsic motivation</b> and <b>goals</b> as two components in motivation.
      Specifically we distinguish between learning goals and performance goals, as indicated by Goal Setting Theory.
      Finally, the expected <b>utility</b> of a task captures its value from a rational cost-benefit analysis perspective. 
      Note that a task may be of high intrinsic motivation yet low utility, e.g., reading a novel.
      </p>

      <h5>Difficulty</h5>

      <p>
      Difficulty is a subjective measure reflecting the cost of performing a task. 
      For delegation, we frame difficulty as the interplay between task requirements and the ability of a person to meet those requirements. 
      Some tasks are difficult because they are time-consuming or laborious; others, because of the required training or expertise. 
      To differentiate the two, we include <b>effort required</b> and <b>expertise required</b> as components in difficulty. The third component, <b>belief about abilities possessed</b>, can also be thought of as task-specific self-confidence (also called self-efficacy) and has been empirically shown to predict allocation strategies between people and automation.
      </p>

      <p>Additionally, we contextualize our difficulty measures with two specific skill requirements: the amount of <b>creativity</b> and <b>social skills</b> required. We choose these because they are considered more difficult for machines than for humans.
      </p>

      <h5>Risk</h5>
      <p>
Real-world tasks involve uncertainty and risk in accomplishing the task, so a rational decision on delegation involves more than just cost and benefit. 
Delegation amounts to a bet: a choice considering the probabilities of accomplishing the goal against the risks and costs of each agent.
Perkins et al. define risk practically as a “probability of harm or loss,” finding that people rely on automation less as the probability of mortality increases. 
Responsibility or accountability may play a role if delegation is seen as a way to share blame. 
We thus decompose risk into the three components of personal <b>accountability</b> for the task outcome; the <b>uncertainty</b>, or the probability of errors; and the scope of <b>impact</b>, or cost or magnitude of those errors.
      </p>

      <h5>Trust</h5>
      <p>
      Trust captures how people deal with risk or uncertainty. 
      We use Lee and See’s definition of trust as “the attitude that an agent will help achieve an individual’s goals in a situation characterized by uncertainty and vulnerability."
Trust is generally regarded as the most salient factor in reliance on automation.
Here, we consider trust as a combination of perceived <b>ability</b> of the AI agent, agent <b>interpretability</b> (ability to explain itself), and perceived <b>value alignment</b>. 
Each of these corresponds to a component of trust in automation in Lee and See: performance, process, and purpose.
      </p>


      <h3>Limitations</h3>

      <p>We stress that this research is a first attempt at quantifying the delegability of different tasks. The four-factor framework we present here -- Trust, Difficulty, Ability, Motivation -- is by no means comprehensive or necessarily the correct approach. We also acknowledge limitations in the administration of our survey on Mechanical Turk as a data collection method, and of the implicit impact of our survey wording and choice of tasks on the results. Finally, we limited the survey respondants to United States residents; we expect a dependence on cultural customs and norms.</p>

      <p>Nevertheless, we suggest that the direction is a valuable one. Quantification of which tasks people actually <i>want</i> delegated and why, alongside the amount and type of machine assistance for each task, would go a long way towards more efficient and responsible allocation of industry and academic research resources. Further, a better understanding of the factors behind what makes one task more delegable to AI than another would improve our ability to address AI shortcomings, improve usability, and generalize task-specific ML, HCI, or HRI results to other tasks in the task space.</p>

      <p>Our hope is that this preliminary work can assist the community in developing more rigorous methods of supporting human-centered machine learning and automation.</p>

      <h3>About Us</h3>
      <p>We are a group of researchers at the University of Colorado, Boulder, interested in human-centered machine learning. This research project was conducted by Professor <a href="https://www.chenhaot.com/">Chenhao Tan</a> and M.S. student <a href="https://blubars.github.io/">Brian Lubars</a>.</p>

    </div> <!-- div container -->
  </body>

</html>
